package org.usfirst.frc.team1895.robot.subsystems;

import org.opencv.core.Mat;
import org.opencv.core.Rect;
import org.opencv.imgproc.Imgproc;
//import org.usfirst.frc.team1897.robot.commands.ExampleCommand;
import org.usfirst.frc.team1895.robot.RobotMap;

//import edu.wpi.cscore.AxisCamera;
import edu.wpi.cscore.CvSink;
import edu.wpi.cscore.CvSource;
import edu.wpi.cscore.UsbCamera;
import edu.wpi.first.wpilibj.CameraServer;
import edu.wpi.first.wpilibj.Relay;
import edu.wpi.first.wpilibj.command.Subsystem;
import edu.wpi.first.wpilibj.smartdashboard.SmartDashboard;

/*
 * This class will process the the images coming in from the camera, send the 
 * images through the GRIP filter pipeline and finally send the processed
 * images to an MJPEGSTREAM in order to view the output on the smart
 * dashboard.
 */

public class FilteredCamera extends Subsystem {

	public void initDefaultCommand() {
		// Set the default command for the subsystem here.
		// setDefaultCommand(new ExampleCommand());
	}

	// The GRIP pipeline class, code was generated by the GRIP tool
	private LiftTrackerRefined gearPipeline;
	private Thread visionThread;

	static double centerX;
	static double lengthBetweenTargets;
	static double angleToTarget;
	static double horizontalOffset;
	static double measuredWidth;
	static double pixelsPerInch;
	double exposure;
	public static final double WIDTH_BETWEEN_TARGET = 8;
	public static final double WIDTH_OF_TAPE = 2; // INCHES
	Relay led_relay;
	
	public FilteredCamera() {
		SmartDashboard.putNumber("Exposure: ", 100.0);
		startGearVisionThread();
		led_relay = new Relay(0);
		led_relay.set(Relay.Value.kOn);
		led_relay.set(Relay.Value.kReverse);
		
	}

	// This method is the true "constructor" of the camera. All we really want
	// is 8to have the robot
	// spawn an individual thread to do the vision processing. The Pipeline was
	// all setup through
	// GRIP.
	//
	// Grab the filtered image from the grip pipeline, overlay a rectangle onto
	// the Mat object and
	// write the image to the output stream that should be available for display
	// on the smart dashboard.
	private void startGearVisionThread() {

		gearPipeline = new LiftTrackerRefined();

		UsbCamera liftPegCamera = CameraServer.getInstance().startAutomaticCapture();
		
//		exposure = SmartDashboard.getNumber("Exposure: ");
		SmartDashboard.putNumber("Read Exposure: ", exposure);
		
		liftPegCamera.setExposureManual(150);

		CvSink cvSink = CameraServer.getInstance().getVideo(); // capture mats
																// from camera
		CvSource outputStream = CameraServer.getInstance().putVideo("Rectangle Stream", 640, 480); // send
																									// stream
																									// to
																									// CameraServer

		Mat mat = new Mat(); // define mat in order to reuse it

		visionThread = new Thread(() -> {

			while (!Thread.interrupted()) { // this should only be false when
											// shutting down
				if (cvSink.grabFrame(mat) == 0) { // fill mat with image from
													// camera TODO exception
													// handling (there is an
													// error if it returns 0)
					outputStream.notifyError(cvSink.getError());    // send an
																	// error
																	// instead
																	// of the
																	// mat
					SmartDashboard.putString("Vision State", "Couldn't grab frame");
					continue; // skip to the next iteration of the thread
				}

				gearPipeline.process(mat); // process the mat (this does not
											// change the mat) and has an
											// internal output to pipeline

				SmartDashboard.putNumber("SIZE --- ", gearPipeline.filterContoursOutput().size());

				if (gearPipeline.filterContoursOutput().size() >= 1) {

					// Should be the rectangle the farthest to the left
					Rect r1 = Imgproc.boundingRect(gearPipeline.filterContoursOutput().get(0));

					centerX = r1.x + (r1.width / 2);

					measuredWidth = r1.width;
					pixelsPerInch = WIDTH_OF_TAPE / measuredWidth;
					horizontalOffset = (centerX - 80) * pixelsPerInch;
					
//					SmartDashboard.putDouble("centerX: ", centerX);
//					SmartDashboard.putDouble("Width in pixels: ", r1.width);
//					SmartDashboard.putDouble("Offset:  ", horizontalOffset);
//					SmartDashboard.putDouble("angle inside the Filtered Camera:  ", Math.toDegrees(Math.atan((horizontalOffset/13))));

				} else {
					SmartDashboard.putString("Vision State", "FAILED TO FIND TARGET");
				}
				outputStream.putFrame(mat); // Give stream (and CameraServer) a
											// new frame

				mat.release();
			}

		});
		visionThread.setDaemon(true);

	}

	public void startVisionThread() {
		visionThread.start();
	}

	public void stopVisionThread() {
		visionThread.suspend();
	}

	public void resumeVisionThread() {
		visionThread.resume();
	}

	private int detectedContours() {
		return gearPipeline.filterContoursOutput().size();
	}

	public double angleToTarget() {
		return angleToTarget;
	}

	public double getCenterX() {
		return centerX;
	}

	public double getOffset() {
		System.out.println("offset ----- " + horizontalOffset);
		return horizontalOffset;
	}
}